"""Various data set implementations."""

import math
import torch
import numpy as np
import pandas as pd
from torch import Tensor
from typing import List, Tuple
from continuity.data import device, tensor, Sensor, Observation, DataSet


class SelfSupervisedDataSet(DataSet):
    """
    A `SelfSupervisedDataSet` is a data set constructed from a set of
    observations that exports batches of observations and labels for
    self-supervised learning. Every data point is created by taking one
    sensor as label.

    Every batch consists of tuples `(u, x, v)`, where `u` is the observation
    tensor, `x` is the label's coordinate and `v` is the label.

    Args:
        observations: List of observations.
        batch_size: Batch size.
        shuffle: Shuffle dataset.
    """

    def __init__(
        self,
        observations: List[Observation],
        batch_size: int,
        shuffle: bool = True,
    ):
        self.observations = observations
        self.batch_size = batch_size

        self.num_sensors = observations[0].num_sensors
        self.coordinate_dim = observations[0].sensors[0].coordinate_dim
        self.num_channels = observations[0].sensors[0].num_channels

        # Check consistency across observations
        for observation in self.observations:
            assert (
                observation.num_sensors == self.num_sensors
            ), "Inconsistent number of sensors."
            assert (
                observation.coordinate_dim == self.coordinate_dim
            ), "Inconsistent coordinate dimension."
            assert (
                observation.num_channels == self.num_channels
            ), "Inconsistent number of channels."

        self.x = []
        self.u = []
        self.y = []
        self.v = []

        for observation in self.observations:
            x, u = observation.to_tensors()

            for sensor in observation.sensors:
                y = tensor(sensor.x).unsqueeze(0)
                v = tensor(sensor.u).unsqueeze(0)

                # Add data point for every sensor
                self.x.append(x)
                self.u.append(u)
                self.y.append(y)
                self.v.append(v)

        self.x = torch.stack(self.x)
        self.u = torch.stack(self.u)
        self.y = torch.stack(self.y)
        self.v = torch.stack(self.v)

        if shuffle:
            idx = torch.randperm(len(self.u))
            self.x = self.x[idx]
            self.u = self.u[idx]
            self.y = self.y[idx]
            self.v = self.v[idx]

        # Move to device
        self.to(device=device)

    def get_observation(self, i: int) -> Observation:
        """Return i-th original observation object.

        Args:
            i: Index of observation.

        Returns:
            Observation object.
        """
        return self.observations[i]

    def __len__(self) -> int:
        """Return number of batches.

        Returns:
            Number of batches.
        """
        return math.ceil(len(self.u) / self.batch_size)

    def __getitem__(self, i: int) -> Tuple[Tensor, Tensor, Tensor]:
        """Return i-th batch as a tuple `(u, x, v)`, where

        - Sensor positions `x` is a tensor of shape `(batch_size, num_sensors, coordinate_dim)`
        - Sensor values `u` is a tensor of shape `(batch_size, num_sensors, num_channels)`
        - Evaluation coordinates `y` is a tensor of shape `(batch_size, 1, coordinate_dim)`
        - Labels `v` is a tensor  of shape `(batch_size, 1, num_channels)`

        Args:
            i: Index of batch.

        Returns:
            Batch tuple `(x, u, y, v)`.
        """
        low = i * self.batch_size
        high = min(low + self.batch_size, len(self.u))
        return self.x[low:high], self.u[low:high], self.y[low:high], self.v[low:high]

    def to(self, device: torch.device):
        """Move data set to device.

        Args:
            device: Torch device dataset is moved to.
        """
        self.x = self.x.to(device)
        self.u = self.u.to(device)
        self.y = self.y.to(device)
        self.v = self.v.to(device)


class Sine(SelfSupervisedDataSet):
    r"""Creates a data set of sine waves.

    The data set is generated by sampling sine waves at the given number of
    sensors placed evenly in the interval $[-1, 1]$. The wave length of the
    sine waves is evenly distributed between $\pi$ for the first observation
    and $2\pi$ for the last observation, respectively.

    - coordinate_dim: 1
    - num_channels: 1

    Args:
        num_sensors: Number of sensors.
        size: Size of data set.
        batch_size: Batch size. Defaults to 32.
    """

    def __init__(self, num_sensors: int, size: int, batch_size: int = 32):
        self.num_sensors = num_sensors
        self.size = size

        self.coordinate_dim = 1
        self.num_channels = 1

        # Generate observations
        observations = [self.generate_observation(i) for i in range(self.size)]

        super().__init__(observations, batch_size)

    def generate_observation(self, i: float):
        """Generate observation

        Args:
            i: Index of observation (0 <= i <= size).
        """
        x = np.linspace(-1, 1, self.num_sensors)

        if self.size == 1:
            w = 1
        else:
            w = i / self.size + 1

        u = np.sin(w * np.pi * x)

        sensors = [Sensor(np.array([x]), np.array([u])) for x, u in zip(x, u)]

        return Observation(sensors)


class Flame(SelfSupervisedDataSet):
    """Turbulent flow samples from flame dataset.

    - coordinate_dim: 2
    - num_channels: 1

    (TODO)
    """

    def __init__(self, size, batch_size):
        self.num_sensors = 16 * 16
        self.size = size

        self.coordinate_dim = 2
        self.num_channels = 1

        # Generate observations
        observations = [self.generate_observation(i) for i in range(self.size)]

        super().__init__(observations, batch_size)

    def generate_observation(self, i: int):
        # Load data
        input_path = "flame/"
        res = "LR"

        df = pd.read_csv(input_path + "train.csv")
        data_path = input_path + f"flowfields/{res}/train"

        filename = df["ux_filename"][i + 1]
        u = np.fromfile(data_path + "/" + filename, dtype="<f4").reshape(16, 16, 1)

        # Normalize
        u = (u - u.mean()) / u.std()

        # Positions
        x = np.stack(
            np.meshgrid(
                np.linspace(-1, 1, 16),
                np.linspace(-1, 1, 16),
            ),
            axis=2,
        )

        # Sensors
        sensors = [Sensor(x[i][j], u[i][j]) for i in range(16) for j in range(16)]
        return Observation(sensors)
