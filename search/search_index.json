{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Continuity Learning function operators with neural networks. <p>Continuity is a Python package for machine learning on functions operators. It implements various neural network architectures, including DeepONets or neural operators, physics-informed loss functions to train the networks based on PDEs, and a variety of benchmarks.</p> <p>Installation</p><p>Steps to install and requirements</p> <p>Learning Operators</p><p>Basics of learning function operators with neural networks </p> <p>Browse the API</p><p>Full documentation of the API</p>"},{"location":"CHANGELOG/","title":"CHANGELOG","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":"<ul> <li>Set up minimal project structure.</li> <li>Create first notebook.</li> </ul>"},{"location":"api/continuity/","title":"API","text":"<p>The Continuity package.</p>"},{"location":"api/continuity/data/","title":"Data","text":"<p>In Continuity, data is given by observations. Every observation is a set of function evaluations, so-called sensors. Every data set is a set of observations, evaluation coordinates and labels.</p>"},{"location":"api/continuity/data/#continuity.data.Sensor","title":"<code>Sensor(x, u)</code>","text":"<p>A sensor is a function evaluation.</p> PARAMETER  DESCRIPTION <code>x</code> <p>spatial coordinate of shape (coordinate_dim)</p> <p> TYPE: <code>ndarray</code> </p> <code>u</code> <p>function value of shape (num_channels)</p> <p> TYPE: <code>ndarray</code> </p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>def __init__(self, x: ndarray, u: ndarray):\nself.x = x\nself.u = u\nself.coordinate_dim = x.shape[0]\nself.num_channels = u.shape[0]\n</code></pre>"},{"location":"api/continuity/data/#continuity.data.Observation","title":"<code>Observation(sensors)</code>","text":"<p>An observation is a set of sensors.</p> PARAMETER  DESCRIPTION <code>sensors</code> <p>List of sensors. Used to derive 'num_sensors', 'coordinate_dim' and 'num_channels'.</p> <p> TYPE: <code>List[Sensor]</code> </p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>def __init__(self, sensors: List[Sensor]):\nself.sensors = sensors\nself.num_sensors = len(sensors)\nassert self.num_sensors &gt; 0\nself.coordinate_dim = self.sensors[0].coordinate_dim\nself.num_channels = self.sensors[0].num_channels\n# Check consistency across sensors\nfor sensor in self.sensors:\nassert (\nsensor.coordinate_dim == self.coordinate_dim\n), \"Inconsistent coordinate dimension.\"\nassert (\nsensor.num_channels == self.num_channels\n), \"Inconsistent number of channels.\"\n</code></pre>"},{"location":"api/continuity/data/#continuity.data.Observation.to_tensors","title":"<code>to_tensors()</code>","text":"<p>Convert observation to tensors.</p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor]</code> <p>Two tensors: The first tensor contains sensor positions of shape (num_sensors, coordinate_dim), the second tensor contains the sensor values of shape (num_sensors, num_channels).</p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>def to_tensors(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n\"\"\"Convert observation to tensors.\n    Returns:\n        Two tensors: The first tensor contains sensor positions of shape (num_sensors, coordinate_dim), the second tensor contains the sensor values of shape (num_sensors, num_channels).\n    \"\"\"\nx = torch.zeros((self.num_sensors, self.coordinate_dim))\nu = torch.zeros((self.num_sensors, self.num_channels))\nfor i, sensor in enumerate(self.sensors):\nx[i] = tensor(sensor.x)\nu[i] = tensor(sensor.u)\n# Move to device\nx.to(device)\nu.to(device)\nreturn x, u\n</code></pre>"},{"location":"api/continuity/data/#continuity.data.DataSet","title":"<code>DataSet</code>","text":"<p>Data set base class.</p>"},{"location":"api/continuity/data/#continuity.data.DataSet.__len__","title":"<code>__len__()</code>  <code>abstractmethod</code>","text":"<p>Return number of batches.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of batches.</p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>@abstractmethod\ndef __len__(self) -&gt; int:\n\"\"\"Return number of batches.\n    Returns:\n        Number of batches.\n    \"\"\"\n</code></pre>"},{"location":"api/continuity/data/#continuity.data.DataSet.__getitem__","title":"<code>__getitem__(i)</code>  <code>abstractmethod</code>","text":"<p>Return i-th batch as a tuple <code>(x, u, y, v)</code> with tensors for sensor positions <code>x</code>, sensor values <code>u</code>, evaluation coordinates <code>y</code> and target labels <code>v</code>.</p> PARAMETER  DESCRIPTION <code>i</code> <p>Index of batch.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Batch tuple <code>(x, u, y, v)</code>.</p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>@abstractmethod\ndef __getitem__(self, i: int) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor]:\n\"\"\"Return i-th batch as a tuple `(x, u, y, v)` with tensors for\n    sensor positions `x`, sensor values `u`, evaluation coordinates `y`\n    and target labels `v`.\n    Args:\n        i: Index of batch.\n    Returns:\n        Batch tuple `(x, u, y, v)`.\n    \"\"\"\n</code></pre>"},{"location":"api/continuity/data/#continuity.data.get_device","title":"<code>get_device()</code>","text":"<p>Get torch device.</p> RETURNS DESCRIPTION <code>device</code> <p>Device.</p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>def get_device() -&gt; torch.device:\n\"\"\"Get torch device.\n    Returns:\n        Device.\n    \"\"\"\ndevice = torch.device(\"cpu\")\n# if torch.backends.mps.is_available():\n#     device = torch.device(\"mps\")\nif torch.cuda.is_available():\ndevice = torch.device(\"cuda\")\nreturn device\n</code></pre>"},{"location":"api/continuity/data/#continuity.data.tensor","title":"<code>tensor(x)</code>","text":"<p>Default conversion for tensors.</p> Source code in <code>src/continuity/data/__init__.py</code> <pre><code>def tensor(x):\n\"\"\"Default conversion for tensors.\"\"\"\nreturn torch.tensor(x, device=device, dtype=torch.float32)\n</code></pre>"},{"location":"api/continuity/data/datasets/","title":"Datasets","text":"<p>Various data set implementations.</p>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.SelfSupervisedDataSet","title":"<code>SelfSupervisedDataSet(observations, batch_size, shuffle=True)</code>","text":"<p>             Bases: <code>DataSet</code></p> <p>A <code>SelfSupervisedDataSet</code> is a data set constructed from a set of observations that exports batches of observations and labels for self-supervised learning. Every data point is created by taking one sensor as label.</p> <p>Every batch consists of tuples <code>(x, u, y, v)</code>, where <code>x is the sensor positions,</code>u<code>is the sensor values,</code>x<code>is the label's coordinate and</code>v` is the label.</p> PARAMETER  DESCRIPTION <code>observations</code> <p>List of observations.</p> <p> TYPE: <code>List[Observation]</code> </p> <code>batch_size</code> <p>Batch size.</p> <p> TYPE: <code>int</code> </p> <code>shuffle</code> <p>Shuffle dataset.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def __init__(\nself,\nobservations: List[Observation],\nbatch_size: int,\nshuffle: bool = True,\n):\nself.observations = observations\nself.batch_size = batch_size\nself.num_sensors = observations[0].num_sensors\nself.coordinate_dim = observations[0].sensors[0].coordinate_dim\nself.num_channels = observations[0].sensors[0].num_channels\n# Check consistency across observations\nfor observation in self.observations:\nassert (\nobservation.num_sensors == self.num_sensors\n), \"Inconsistent number of sensors.\"\nassert (\nobservation.coordinate_dim == self.coordinate_dim\n), \"Inconsistent coordinate dimension.\"\nassert (\nobservation.num_channels == self.num_channels\n), \"Inconsistent number of channels.\"\nself.x = []\nself.u = []\nself.y = []\nself.v = []\nfor observation in self.observations:\nx, u = observation.to_tensors()\nfor sensor in observation.sensors:\ny = tensor(sensor.x).unsqueeze(0)\nv = tensor(sensor.u).unsqueeze(0)\n# Add data point for every sensor\nself.x.append(x)\nself.u.append(u)\nself.y.append(y)\nself.v.append(v)\nself.x = torch.stack(self.x)\nself.u = torch.stack(self.u)\nself.y = torch.stack(self.y)\nself.v = torch.stack(self.v)\nif shuffle:\nidx = torch.randperm(len(self.u))\nself.x = self.x[idx]\nself.u = self.u[idx]\nself.y = self.y[idx]\nself.v = self.v[idx]\n# Move to device\nself.to(device=device)\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.SelfSupervisedDataSet.get_observation","title":"<code>get_observation(i)</code>","text":"<p>Return i-th original observation object.</p> PARAMETER  DESCRIPTION <code>i</code> <p>Index of observation.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Observation</code> <p>Observation object.</p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def get_observation(self, i: int) -&gt; Observation:\n\"\"\"Return i-th original observation object.\n    Args:\n        i: Index of observation.\n    Returns:\n        Observation object.\n    \"\"\"\nreturn self.observations[i]\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.SelfSupervisedDataSet.__len__","title":"<code>__len__()</code>","text":"<p>Return number of batches.</p> RETURNS DESCRIPTION <code>int</code> <p>Number of batches.</p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return number of batches.\n    Returns:\n        Number of batches.\n    \"\"\"\nreturn math.ceil(len(self.u) / self.batch_size)\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.SelfSupervisedDataSet.__getitem__","title":"<code>__getitem__(i)</code>","text":"<p>Return i-th batch as a tuple <code>(x, u, y, v)</code>, where</p> <ul> <li>Sensor positions <code>x</code> is a tensor of shape <code>(batch_size, num_sensors, coordinate_dim)</code></li> <li>Sensor values <code>u</code> is a tensor of shape <code>(batch_size, num_sensors, num_channels)</code></li> <li>Evaluation coordinates <code>y</code> is a tensor of shape <code>(batch_size, 1, coordinate_dim)</code></li> <li>Labels <code>v</code> is a tensor  of shape <code>(batch_size, 1, num_channels)</code></li> </ul> PARAMETER  DESCRIPTION <code>i</code> <p>Index of batch.</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Batch tuple <code>(x, u, y, v)</code>.</p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def __getitem__(self, i: int) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor]:\n\"\"\"Return i-th batch as a tuple `(x, u, y, v)`, where\n    - Sensor positions `x` is a tensor of shape `(batch_size, num_sensors, coordinate_dim)`\n    - Sensor values `u` is a tensor of shape `(batch_size, num_sensors, num_channels)`\n    - Evaluation coordinates `y` is a tensor of shape `(batch_size, 1, coordinate_dim)`\n    - Labels `v` is a tensor  of shape `(batch_size, 1, num_channels)`\n    Args:\n        i: Index of batch.\n    Returns:\n        Batch tuple `(x, u, y, v)`.\n    \"\"\"\nlow = i * self.batch_size\nhigh = min(low + self.batch_size, len(self.u))\nreturn self.x[low:high], self.u[low:high], self.y[low:high], self.v[low:high]\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.SelfSupervisedDataSet.to","title":"<code>to(device)</code>","text":"<p>Move data set to device.</p> PARAMETER  DESCRIPTION <code>device</code> <p>Torch device dataset is moved to.</p> <p> TYPE: <code>device</code> </p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def to(self, device: torch.device):\n\"\"\"Move data set to device.\n    Args:\n        device: Torch device dataset is moved to.\n    \"\"\"\nself.x = self.x.to(device)\nself.u = self.u.to(device)\nself.y = self.y.to(device)\nself.v = self.v.to(device)\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.Sine","title":"<code>Sine(num_sensors, size, batch_size=32)</code>","text":"<p>             Bases: <code>SelfSupervisedDataSet</code></p> <p>Creates a data set of sine waves.</p> <p>The data set is generated by sampling sine waves at the given number of sensors placed evenly in the interval \\([-1, 1]\\). The wave length of the sine waves is evenly distributed between \\(\\pi\\) for the first observation and \\(2\\pi\\) for the last observation, respectively.</p> <p>The <code>Sine</code> dataset generates \\(N\\) sine waves $$ f(x) = \\sin(w_k x), \\quad w_k = 1 + \\frac{k}{N-1}, \\quad k = 0, \\dots, N-1. $$ As a <code>SelfSupervisedDataset</code> it exports batches of samples for self-supervised training, namely $$ \\left(\\mathbf{x}, f(\\mathbf{x}), x_j, f(x_j)\\right), \\quad \\text{for } j = 1, \\dots, M, $$ where \\(\\mathbf{x} = (x_i)_{i=1 \\dots M}\\) are the \\(M\\) equidistantly distributed sensor positions.</p> <ul> <li>coordinate_dim: 1</li> <li>num_channels: 1</li> </ul> PARAMETER  DESCRIPTION <code>num_sensors</code> <p>Number of sensors.</p> <p> TYPE: <code>int</code> </p> <code>size</code> <p>Size of data set.</p> <p> TYPE: <code>int</code> </p> <code>batch_size</code> <p>Batch size. Defaults to 32.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def __init__(self, num_sensors: int, size: int, batch_size: int = 32):\nself.num_sensors = num_sensors\nself.size = size\nself.coordinate_dim = 1\nself.num_channels = 1\n# Generate observations\nobservations = [self.generate_observation(i) for i in range(self.size)]\nsuper().__init__(observations, batch_size)\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.Sine.generate_observation","title":"<code>generate_observation(i)</code>","text":"<p>Generate observation</p> PARAMETER  DESCRIPTION <code>i</code> <p>Index of observation (0 &lt;= i &lt;= size).</p> <p> TYPE: <code>float</code> </p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def generate_observation(self, i: float):\n\"\"\"Generate observation\n    Args:\n        i: Index of observation (0 &lt;= i &lt;= size).\n    \"\"\"\nx = np.linspace(-1, 1, self.num_sensors)\nif self.size == 1:\nw = 1\nelse:\nw = 1 + i / (self.size - 1)\nu = np.sin(w * np.pi * x)\nsensors = [Sensor(np.array([x]), np.array([u])) for x, u in zip(x, u)]\nreturn Observation(sensors)\n</code></pre>"},{"location":"api/continuity/data/datasets/#continuity.data.datasets.Flame","title":"<code>Flame(size, batch_size)</code>","text":"<p>             Bases: <code>SelfSupervisedDataSet</code></p> <p>Turbulent flow samples from flame dataset.</p> <ul> <li>coordinate_dim: 2</li> <li>num_channels: 1</li> </ul> <p>(TODO)</p> Source code in <code>src/continuity/data/datasets.py</code> <pre><code>def __init__(self, size, batch_size):\nself.num_sensors = 16 * 16\nself.size = size\nself.coordinate_dim = 2\nself.num_channels = 1\n# Generate observations\nobservations = [self.generate_observation(i) for i in range(self.size)]\nsuper().__init__(observations, batch_size)\n</code></pre>"},{"location":"api/continuity/data/losses/","title":"Losses","text":"<p>Loss functions.</p>"},{"location":"api/continuity/data/losses/#continuity.data.losses.PDE","title":"<code>PDE</code>","text":"<p>PDE base class.</p>"},{"location":"api/continuity/data/losses/#continuity.data.losses.PDE.__call__","title":"<code>__call__(op, x, u, y, v)</code>  <code>abstractmethod</code>","text":"<p>Computes PDE loss.</p> Source code in <code>src/continuity/data/losses.py</code> <pre><code>@abstractmethod\ndef __call__(self, op, x: Tensor, u: Tensor, y: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Computes PDE loss.\"\"\"\n</code></pre>"},{"location":"api/continuity/data/losses/#continuity.data.losses.PhysicsInformedLoss","title":"<code>PhysicsInformedLoss(pde)</code>","text":"<p>Physics-informed loss function for training operators in Continuity.</p> PARAMETER  DESCRIPTION <code>pde</code> <p>Maps evaluation coordinates \\(y\\) and callable \\(v\\) to PDE loss.</p> <p> TYPE: <code>PDE</code> </p> Source code in <code>src/continuity/data/losses.py</code> <pre><code>def __init__(self, pde: PDE):\nself.pde = pde\n</code></pre>"},{"location":"api/continuity/data/losses/#continuity.data.losses.PhysicsInformedLoss.__call__","title":"<code>__call__(op, x, u, y, v)</code>","text":"<p>Evaluate loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object</p> <p> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Ignored</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuity/data/losses.py</code> <pre><code>def __call__(self, op, x: Tensor, u: Tensor, y: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Evaluate loss.\n    Args:\n        op: Operator object\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of evaluation coordinates of shape (batch_size, x_size, coordinate_dim)\n        v: Ignored\n    \"\"\"\n# Call operator\nv_pred = op(x, u, y)\n# Get pde loss\nreturn self.pde(x, u, y, v_pred)\n</code></pre>"},{"location":"api/continuity/operators/","title":"Operators","text":"<p>Operators in Continuity.</p>"},{"location":"api/continuity/operators/#continuity.operators.Operator","title":"<code>Operator</code>","text":"<p>             Bases: <code>Module</code></p> <p>Operator base class.</p> <p>An operator is a neural network model that maps functions by mapping an observation to the evaluations of the mapped function at given coordinates.</p> <p>This class implements default <code>compile</code> and <code>fit</code> methods.</p>"},{"location":"api/continuity/operators/#continuity.operators.Operator.forward","title":"<code>forward(x, u, y)</code>  <code>abstractmethod</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, x_size, num_channels)</p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>@abstractmethod\ndef forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, x_size, num_channels)\n    \"\"\"\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.Operator.compile","title":"<code>compile(optimizer, loss_fn=None)</code>","text":"<p>Compile operator.</p> PARAMETER  DESCRIPTION <code>optimizer</code> <p>Torch-like optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>Loss function taking (x, u, y, v). Defaults to MSELoss.</p> <p> TYPE: <code>Optional[Loss]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>def compile(self, optimizer: torch.optim.Optimizer, loss_fn: Optional[Loss] = None):\n\"\"\"Compile operator.\n    Args:\n        optimizer: Torch-like optimizer.\n        loss_fn: Loss function taking (x, u, y, v). Defaults to MSELoss.\n    \"\"\"\nself.optimizer = optimizer\nself.loss_fn = loss_fn or MSELoss()\n# Move to device\nself.to(device)\n# Print number of model parameters\nnum_params = sum(p.numel() for p in self.parameters())\nprint(f\"Model parameters: {num_params}\")\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.Operator.fit","title":"<code>fit(dataset, epochs, writer=None)</code>","text":"<p>Fit operator to data set.</p> PARAMETER  DESCRIPTION <code>dataset</code> <p>Data set.</p> <p> TYPE: <code>DataSet</code> </p> <code>epochs</code> <p>Number of epochs.</p> <p> TYPE: <code>int</code> </p> <code>writer</code> <p>Tensorboard-like writer for loss visualization.</p> <p> TYPE: <code>Optional[SummaryWriter]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>def fit(\nself, dataset: DataSet, epochs: int, writer: Optional[SummaryWriter] = None\n):\n\"\"\"Fit operator to data set.\n    Args:\n        dataset: Data set.\n        epochs: Number of epochs.\n        writer: Tensorboard-like writer for loss visualization.\n    \"\"\"\nfor epoch in range(epochs + 1):\nmean_loss = 0\nstart = time()\nfor i in range(len(dataset)):\nx, u, y, v = dataset[i]\ndef closure(x=x, u=u, y=y, v=v):\nself.optimizer.zero_grad()\nloss = self.loss_fn(self, x, u, y, v)\nloss.backward(retain_graph=True)\nreturn loss\nself.optimizer.step(closure)\nself.optimizer.param_groups[0][\"lr\"] *= 0.999\n# Compute mean loss\nmean_loss += self.loss_fn(self, x, u, y, v).detach().item()\nend = time()\nmean_loss /= len(dataset)\nif writer is not None:\nwriter.add_scalar(\"Loss/train\", mean_loss, epoch)\niter_per_second = len(dataset) / (end - start)\nprint(\nf\"\\rEpoch {epoch}:  loss = {mean_loss:.4e}  \"\nf\"({iter_per_second:.2f} it/s)\",\nend=\"\",\n)\nprint(\"\")\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.Operator.loss","title":"<code>loss(x, u, y, v)</code>","text":"<p>Evaluate loss function.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, x_size, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>def loss(self, x: Tensor, u: Tensor, y: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Evaluate loss function.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)\n        v: Tensor of labels of shape (batch_size, x_size, num_channels)\n    \"\"\"\nreturn self.loss_fn(self, x, u, y, v)\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.DeepONet","title":"<code>DeepONet(num_sensors, coordinate_dim=1, num_channels=1, branch_width=32, branch_depth=3, trunk_width=32, trunk_depth=3, basis_functions=8)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Maps continuous functions given as observation to another continuous functions and returns point-wise evaluations. The architecture is inspired by the universal approximation theorem for operators.</p> <p>Reference: Lu Lu et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat Mach Intell 3 218-229 (2021)</p> <p>Note: This operator is not discretization invariant, i.e., it assumes that all observations were evaluated at the same positions.</p> PARAMETER  DESCRIPTION <code>num_sensors</code> <p>Number of sensors (fixed!)</p> <p> TYPE: <code>int</code> </p> <code>coordinate_dim</code> <p>Dimension of coordinate space</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>branch_width</code> <p>Width of branch network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>branch_depth</code> <p>Depth of branch network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>trunk_width</code> <p>Width of trunk network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>trunk_depth</code> <p>Depth of trunk network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>basis_functions</code> <p>Number of basis functions</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> Source code in <code>src/continuity/operators/deeponet.py</code> <pre><code>def __init__(\nself,\nnum_sensors: int,\ncoordinate_dim: int = 1,\nnum_channels: int = 1,\nbranch_width: int = 32,\nbranch_depth: int = 3,\ntrunk_width: int = 32,\ntrunk_depth: int = 3,\nbasis_functions: int = 8,\n):\nsuper().__init__()\nself.coordinate_dim = coordinate_dim\nself.num_channels = num_channels\nself.num_sensors = num_sensors\nself.basis_functions = basis_functions\nbranch_input = num_sensors * num_channels\ntrunk_input = coordinate_dim\nself.branch = DeepResidualNetwork(\nbranch_input,\nself.num_channels * basis_functions,\nbranch_width,\nbranch_depth,\n)\nself.trunk = DeepResidualNetwork(\ntrunk_input,\nself.num_channels * basis_functions,\ntrunk_width,\ntrunk_depth,\n)\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.DeepONet.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape ([batch_size,] num_sensors, [num_channels]). If len(u.shape) &lt; 3, a batch dimension will be added.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape ([batch_size,] y_size, [coordinate_dim]). If len(y.shape) &lt; 3, a batch dimension will be added.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)</p> Source code in <code>src/continuity/operators/deeponet.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Ignored.\n        u: Tensor of sensor values of shape ([batch_size,] num_sensors, [num_channels]). If len(u.shape) &lt; 3, a batch dimension will be added.\n        y: Tensor of coordinates where the mapped function is evaluated of shape ([batch_size,] y_size, [coordinate_dim]). If len(y.shape) &lt; 3, a batch dimension will be added.\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)\n    \"\"\"\n# Unsqueeze if no batch dim\nif len(u.shape) &lt; 3:\nbatch_size = 1\nu = u.unsqueeze(0)\ny = y.unsqueeze(0)\n# Get batch size\nbatch_size = u.shape[0]\n# Get number of evaluations\nassert len(y.shape) &gt;= 2\ny_size = y.shape[1]\n# Check shapes\nassert y.shape[0] == batch_size\nassert u.shape[1] == self.num_sensors\nif len(u.shape) &gt; 2:\nassert u.shape[2] == self.coordinate_dim\n# Reshape branch and trunk inputs\nu = u.reshape((batch_size, -1))\ny = y.reshape((-1, self.coordinate_dim))\n# Pass trough branch and trunk networks\nb = self.branch(u)\nt = self.trunk(y)\n# Compute dot product\nb = b.reshape((batch_size, self.basis_functions, self.num_channels))\nt = t.reshape((batch_size, y_size, self.basis_functions, self.num_channels))\nsum = torch.einsum(\"ubc,uxbc-&gt;uxc\", b, t)\nassert sum.shape == (batch_size, y_size, self.num_channels)\nreturn sum\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.ContinuousConvolution","title":"<code>ContinuousConvolution(kernel, coordinate_dim=1, num_channels=1)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Continuous convolution.</p> <p>Maps continuous functions via continuous convolution with a kernel function to another continuous functions and returns point-wise evaluations.</p> <p>In mathematical terms, for some given \\(y\\), we obtain $$ v(y) = \\int u(x)~\\kappa(x, y)~dx     \\approx \\frac{1}{N} \\sum_{i=1}^{N} u_i~\\kappa(x_i, y) $$ where \\((x_i, u_i)\\) are the \\(N\\) sensors of the mapped observation.</p> PARAMETER  DESCRIPTION <code>kernel</code> <p>Kernel function \\(\\kappa\\) or network (if \\(d\\) is the coordinate dimension, \\(\\kappa: \\R^d \\times \\R^d \\to \\R\\))</p> <p> TYPE: <code>Union[Callable[[Tensor], Tensor], Module]</code> </p> <code>coordinate_dim</code> <p>Dimension of coordinate space</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def __init__(\nself,\nkernel: Union[Callable[[Tensor], Tensor], torch.nn.Module],\ncoordinate_dim: int = 1,\nnum_channels: int = 1,\n):\nsuper().__init__()\nself.kernel = kernel\nself.coordinate_dim = coordinate_dim\nself.num_channels = num_channels\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.ContinuousConvolution.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)</p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)\n    \"\"\"\n# Unsqueeze if no batch dim\nif len(u.shape) &lt; 3:\nbatch_size = 1\nx = x.unsqueeze(0)\nu = u.unsqueeze(0)\ny = y.unsqueeze(0)\n# Patch for 1D coordinates and one channel\nif len(x.shape) &lt; 3 and self.coordinate_dim == 1:\nx = x.unsqueeze(-1)\nif len(u.shape) &lt; 3 and self.num_channels == 1:\nu = u.unsqueeze(-1)\nif len(y.shape) &lt; 3 and self.coordinate_dim == 1:\ny = y.unsqueeze(-1)\n# Get batch size etc.\nbatch_size = u.shape[0]\nnum_sensors = u.shape[1]\ny_size = y.shape[1]\n# Check shapes\nassert x.shape[0] == batch_size\nassert y.shape[0] == batch_size\nassert x.shape[1] == num_sensors\n# Apply the kernel function\nx_expanded = x.unsqueeze(2)\ny_expanded = y.unsqueeze(1)\nk = self.kernel(x_expanded, y_expanded)\nassert k.shape == (batch_size, num_sensors, y_size)\n# Compute integral\nintegral = torch.einsum(\"bsy,bsc-&gt;byc\", k, u) / num_sensors\nassert integral.shape == (batch_size, y_size, self.num_channels)\nreturn integral\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.NeuralOperator","title":"<code>NeuralOperator(coordinate_dim=1, num_channels=1, depth=1, kernel_width=32, kernel_depth=3)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Neural operator architecture</p> <p>Maps continuous functions given as observation to another continuous functions and returns point-wise evaluations. The architecture is a stack of continuous convolutions with a lifting layer and a projection layer.</p> <p>Reference: N. Kovachki et al. Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs. JMLR 24 1-97 (2023)</p> <p>For now, sensors positions are equal across all layers.</p> PARAMETER  DESCRIPTION <code>coordinate_dim</code> <p>Dimension of coordinate space</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>depth</code> <p>Number of hidden layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>kernel_width</code> <p>Width of kernel network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>kernel_depth</code> <p>Depth of kernel network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def __init__(\nself,\ncoordinate_dim: int = 1,\nnum_channels: int = 1,\ndepth: int = 1,\nkernel_width: int = 32,\nkernel_depth: int = 3,\n):\nsuper().__init__()\nself.coordinate_dim = coordinate_dim\nself.num_channels = num_channels\nself.lifting = ContinuousConvolution(\nNeuralNetworkKernel(kernel_width, kernel_depth),\ncoordinate_dim,\nnum_channels,\n)\nself.hidden_layers = torch.nn.ModuleList(\n[\nContinuousConvolution(\nNeuralNetworkKernel(kernel_width, kernel_depth),\ncoordinate_dim,\nnum_channels,\n)\nfor _ in range(depth)\n]\n)\nself.projection = ContinuousConvolution(\nNeuralNetworkKernel(kernel_width, kernel_depth),\ncoordinate_dim,\nnum_channels,\n)\n</code></pre>"},{"location":"api/continuity/operators/#continuity.operators.NeuralOperator.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)</p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim).\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels).\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)\n    \"\"\"\n# Unsqueeze if no batch dim\nif len(u.shape) &lt; 3:\nbatch_size = 1\nx = x.unsqueeze(0)\nu = u.unsqueeze(0)\ny = y.unsqueeze(0)\n# Patch for 1D coordinates and one channel\nif len(x.shape) &lt; 3 and self.coordinate_dim == 1:\nx = x.unsqueeze(-1)\nif len(u.shape) &lt; 3 and self.num_channels == 1:\nu = u.unsqueeze(-1)\nif len(y.shape) &lt; 3 and self.coordinate_dim == 1:\ny = y.unsqueeze(-1)\n# Get batch size etc.\nbatch_size = u.shape[0]\nnum_sensors = u.shape[1]\ny_size = y.shape[1]\n# Check shapes\nassert x.shape[0] == batch_size\nassert y.shape[0] == batch_size\nassert x.shape[1] == num_sensors\n# Lifting layer (we use x as evaluation coordinates for now)\nv = self.lifting(x, u, x)\nassert v.shape == (batch_size, num_sensors, self.num_channels)\n# Hidden layers\nfor layer in self.hidden_layers:\n# Layer operation (with residual connection)\nv = layer(x, v, x) + v\nassert v.shape == (batch_size, num_sensors, self.num_channels)\n# Activation\nv = torch.tanh(v)\n# Projection layer\nw = self.projection(x, v, y)\nassert w.shape == (batch_size, y_size, self.num_channels)\nreturn w\n</code></pre>"},{"location":"api/continuity/operators/common/","title":"Common","text":""},{"location":"api/continuity/operators/common/#continuity.operators.common.ResidualLayer","title":"<code>ResidualLayer(width)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Residual layer.</p> PARAMETER  DESCRIPTION <code>width</code> <p>Width of the layer.</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/continuity/operators/common.py</code> <pre><code>def __init__(self, width: int):\nsuper().__init__()\nself.layer = torch.nn.Linear(width, width)\nself.act = torch.nn.Tanh()\n</code></pre>"},{"location":"api/continuity/operators/common/#continuity.operators.common.ResidualLayer.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuity/operators/common.py</code> <pre><code>def forward(self, x: Tensor):\n\"\"\"Forward pass.\"\"\"\nreturn self.act(self.layer(x)) + x\n</code></pre>"},{"location":"api/continuity/operators/common/#continuity.operators.common.DeepResidualNetwork","title":"<code>DeepResidualNetwork(input_size, output_size, width, depth)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Deep residual network.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Size of input tensor</p> <p> TYPE: <code>int</code> </p> <code>output_size</code> <p>Size of output tensor</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>Width of hidden layers</p> <p> TYPE: <code>int</code> </p> <code>depth</code> <p>Number of hidden layers</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/continuity/operators/common.py</code> <pre><code>def __init__(\nself,\ninput_size: int,\noutput_size: int,\nwidth: int,\ndepth: int,\n):\nsuper().__init__()\nself.first_layer = torch.nn.Linear(input_size, width)\nself.hidden_layers = torch.nn.ModuleList(\n[ResidualLayer(width) for _ in range(depth)]\n)\nself.last_layer = torch.nn.Linear(width, output_size)\n</code></pre>"},{"location":"api/continuity/operators/common/#continuity.operators.common.DeepResidualNetwork.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass.</p> Source code in <code>src/continuity/operators/common.py</code> <pre><code>def forward(self, x):\n\"\"\"Forward pass.\"\"\"\nx = self.first_layer(x)\nfor layer in self.hidden_layers:\nx = layer(x)\nreturn self.last_layer(x)\n</code></pre>"},{"location":"api/continuity/operators/common/#continuity.operators.common.NeuralNetworkKernel","title":"<code>NeuralNetworkKernel(kernel_width, kernel_depth)</code>","text":"<p>             Bases: <code>Module</code></p> <p>Neural network kernel.</p> PARAMETER  DESCRIPTION <code>kernel_width</code> <p>Width of kernel network</p> <p> TYPE: <code>int</code> </p> <code>kernel_depth</code> <p>Depth of kernel network</p> <p> TYPE: <code>int</code> </p> Source code in <code>src/continuity/operators/common.py</code> <pre><code>def __init__(\nself,\nkernel_width: int,\nkernel_depth: int,\n):\nsuper().__init__()\nself.net = DeepResidualNetwork(\n1,\n1,\nkernel_width,\nkernel_depth,\n)\n</code></pre>"},{"location":"api/continuity/operators/common/#continuity.operators.common.NeuralNetworkKernel.forward","title":"<code>forward(x, y)</code>","text":"<p>Compute kernel value.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of shape (..., coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of shape (..., coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of shape (...)</p> Source code in <code>src/continuity/operators/common.py</code> <pre><code>def forward(self, x: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Compute kernel value.\n    Args:\n        x: Tensor of shape (..., coordinate_dim)\n        y: Tensor of shape (..., coordinate_dim)\n    Returns:\n        Tensor of shape (...)\n    \"\"\"\nr = ((x - y) ** 2).sum(dim=-1)\noutput_shape = r.shape\nr = r.reshape((-1, 1))\nk = self.net(r)\nk = k.reshape(output_shape)\nreturn k\n</code></pre>"},{"location":"api/continuity/operators/deeponet/","title":"Deeponet","text":"<p>The DeepONet architecture.</p>"},{"location":"api/continuity/operators/deeponet/#continuity.operators.deeponet.DeepONet","title":"<code>DeepONet(num_sensors, coordinate_dim=1, num_channels=1, branch_width=32, branch_depth=3, trunk_width=32, trunk_depth=3, basis_functions=8)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Maps continuous functions given as observation to another continuous functions and returns point-wise evaluations. The architecture is inspired by the universal approximation theorem for operators.</p> <p>Reference: Lu Lu et al. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. Nat Mach Intell 3 218-229 (2021)</p> <p>Note: This operator is not discretization invariant, i.e., it assumes that all observations were evaluated at the same positions.</p> PARAMETER  DESCRIPTION <code>num_sensors</code> <p>Number of sensors (fixed!)</p> <p> TYPE: <code>int</code> </p> <code>coordinate_dim</code> <p>Dimension of coordinate space</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>branch_width</code> <p>Width of branch network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>branch_depth</code> <p>Depth of branch network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>trunk_width</code> <p>Width of trunk network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>trunk_depth</code> <p>Depth of trunk network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>basis_functions</code> <p>Number of basis functions</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> Source code in <code>src/continuity/operators/deeponet.py</code> <pre><code>def __init__(\nself,\nnum_sensors: int,\ncoordinate_dim: int = 1,\nnum_channels: int = 1,\nbranch_width: int = 32,\nbranch_depth: int = 3,\ntrunk_width: int = 32,\ntrunk_depth: int = 3,\nbasis_functions: int = 8,\n):\nsuper().__init__()\nself.coordinate_dim = coordinate_dim\nself.num_channels = num_channels\nself.num_sensors = num_sensors\nself.basis_functions = basis_functions\nbranch_input = num_sensors * num_channels\ntrunk_input = coordinate_dim\nself.branch = DeepResidualNetwork(\nbranch_input,\nself.num_channels * basis_functions,\nbranch_width,\nbranch_depth,\n)\nself.trunk = DeepResidualNetwork(\ntrunk_input,\nself.num_channels * basis_functions,\ntrunk_width,\ntrunk_depth,\n)\n</code></pre>"},{"location":"api/continuity/operators/deeponet/#continuity.operators.deeponet.DeepONet.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Ignored.</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape ([batch_size,] num_sensors, [num_channels]). If len(u.shape) &lt; 3, a batch dimension will be added.</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape ([batch_size,] y_size, [coordinate_dim]). If len(y.shape) &lt; 3, a batch dimension will be added.</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)</p> Source code in <code>src/continuity/operators/deeponet.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Ignored.\n        u: Tensor of sensor values of shape ([batch_size,] num_sensors, [num_channels]). If len(u.shape) &lt; 3, a batch dimension will be added.\n        y: Tensor of coordinates where the mapped function is evaluated of shape ([batch_size,] y_size, [coordinate_dim]). If len(y.shape) &lt; 3, a batch dimension will be added.\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)\n    \"\"\"\n# Unsqueeze if no batch dim\nif len(u.shape) &lt; 3:\nbatch_size = 1\nu = u.unsqueeze(0)\ny = y.unsqueeze(0)\n# Get batch size\nbatch_size = u.shape[0]\n# Get number of evaluations\nassert len(y.shape) &gt;= 2\ny_size = y.shape[1]\n# Check shapes\nassert y.shape[0] == batch_size\nassert u.shape[1] == self.num_sensors\nif len(u.shape) &gt; 2:\nassert u.shape[2] == self.coordinate_dim\n# Reshape branch and trunk inputs\nu = u.reshape((batch_size, -1))\ny = y.reshape((-1, self.coordinate_dim))\n# Pass trough branch and trunk networks\nb = self.branch(u)\nt = self.trunk(y)\n# Compute dot product\nb = b.reshape((batch_size, self.basis_functions, self.num_channels))\nt = t.reshape((batch_size, y_size, self.basis_functions, self.num_channels))\nsum = torch.einsum(\"ubc,uxbc-&gt;uxc\", b, t)\nassert sum.shape == (batch_size, y_size, self.num_channels)\nreturn sum\n</code></pre>"},{"location":"api/continuity/operators/losses/","title":"Losses","text":"<p>Loss functions.</p>"},{"location":"api/continuity/operators/losses/#continuity.operators.losses.Loss","title":"<code>Loss</code>","text":"<p>Loss function for training operators in Continuity.</p>"},{"location":"api/continuity/operators/losses/#continuity.operators.losses.Loss.__call__","title":"<code>__call__(op, x, u, y, v)</code>  <code>abstractmethod</code>","text":"<p>Evaluate loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object</p> <p> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuity/operators/losses.py</code> <pre><code>@abstractmethod\ndef __call__(self, op, x: Tensor, u: Tensor, y: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Evaluate loss.\n    Args:\n        op: Operator object\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of evaluation coordinates of shape (batch_size, x_size, coordinate_dim)\n        v: Tensor of labels of shape (batch_size, x_size, coordinate_dim)\n    \"\"\"\n</code></pre>"},{"location":"api/continuity/operators/losses/#continuity.operators.losses.MSELoss","title":"<code>MSELoss()</code>","text":"<p>             Bases: <code>Loss</code></p> <p>Mean-squared error loss for supervised training.</p> Source code in <code>src/continuity/operators/losses.py</code> <pre><code>def __init__(self):\nself.mse = torch.nn.MSELoss()\n</code></pre>"},{"location":"api/continuity/operators/losses/#continuity.operators.losses.MSELoss.__call__","title":"<code>__call__(op, x, u, y, v)</code>","text":"<p>Evaluate MSE loss.</p> PARAMETER  DESCRIPTION <code>op</code> <p>Operator object</p> <p> </p> <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of evaluation coordinates of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuity/operators/losses.py</code> <pre><code>def __call__(self, op, x: Tensor, u: Tensor, y: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Evaluate MSE loss.\n    Args:\n        op: Operator object\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of evaluation coordinates of shape (batch_size, x_size, coordinate_dim)\n        v: Tensor of labels of shape (batch_size, x_size, coordinate_dim)\n    \"\"\"\n# Call operator\nv_pred = op(x, u, y)\n# Align shapes\nv_pred = v_pred.reshape(v.shape)\n# Return MSE\nreturn self.mse(v_pred, v)\n</code></pre>"},{"location":"api/continuity/operators/neuraloperator/","title":"Neuraloperator","text":"<p>Neural operator.</p>"},{"location":"api/continuity/operators/neuraloperator/#continuity.operators.neuraloperator.ContinuousConvolution","title":"<code>ContinuousConvolution(kernel, coordinate_dim=1, num_channels=1)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Continuous convolution.</p> <p>Maps continuous functions via continuous convolution with a kernel function to another continuous functions and returns point-wise evaluations.</p> <p>In mathematical terms, for some given \\(y\\), we obtain $$ v(y) = \\int u(x)~\\kappa(x, y)~dx     \\approx \\frac{1}{N} \\sum_{i=1}^{N} u_i~\\kappa(x_i, y) $$ where \\((x_i, u_i)\\) are the \\(N\\) sensors of the mapped observation.</p> PARAMETER  DESCRIPTION <code>kernel</code> <p>Kernel function \\(\\kappa\\) or network (if \\(d\\) is the coordinate dimension, \\(\\kappa: \\R^d \\times \\R^d \\to \\R\\))</p> <p> TYPE: <code>Union[Callable[[Tensor], Tensor], Module]</code> </p> <code>coordinate_dim</code> <p>Dimension of coordinate space</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def __init__(\nself,\nkernel: Union[Callable[[Tensor], Tensor], torch.nn.Module],\ncoordinate_dim: int = 1,\nnum_channels: int = 1,\n):\nsuper().__init__()\nself.kernel = kernel\nself.coordinate_dim = coordinate_dim\nself.num_channels = num_channels\n</code></pre>"},{"location":"api/continuity/operators/neuraloperator/#continuity.operators.neuraloperator.ContinuousConvolution.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)</p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)\n    \"\"\"\n# Unsqueeze if no batch dim\nif len(u.shape) &lt; 3:\nbatch_size = 1\nx = x.unsqueeze(0)\nu = u.unsqueeze(0)\ny = y.unsqueeze(0)\n# Patch for 1D coordinates and one channel\nif len(x.shape) &lt; 3 and self.coordinate_dim == 1:\nx = x.unsqueeze(-1)\nif len(u.shape) &lt; 3 and self.num_channels == 1:\nu = u.unsqueeze(-1)\nif len(y.shape) &lt; 3 and self.coordinate_dim == 1:\ny = y.unsqueeze(-1)\n# Get batch size etc.\nbatch_size = u.shape[0]\nnum_sensors = u.shape[1]\ny_size = y.shape[1]\n# Check shapes\nassert x.shape[0] == batch_size\nassert y.shape[0] == batch_size\nassert x.shape[1] == num_sensors\n# Apply the kernel function\nx_expanded = x.unsqueeze(2)\ny_expanded = y.unsqueeze(1)\nk = self.kernel(x_expanded, y_expanded)\nassert k.shape == (batch_size, num_sensors, y_size)\n# Compute integral\nintegral = torch.einsum(\"bsy,bsc-&gt;byc\", k, u) / num_sensors\nassert integral.shape == (batch_size, y_size, self.num_channels)\nreturn integral\n</code></pre>"},{"location":"api/continuity/operators/neuraloperator/#continuity.operators.neuraloperator.NeuralOperator","title":"<code>NeuralOperator(coordinate_dim=1, num_channels=1, depth=1, kernel_width=32, kernel_depth=3)</code>","text":"<p>             Bases: <code>Operator</code></p> <p>Neural operator architecture</p> <p>Maps continuous functions given as observation to another continuous functions and returns point-wise evaluations. The architecture is a stack of continuous convolutions with a lifting layer and a projection layer.</p> <p>Reference: N. Kovachki et al. Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs. JMLR 24 1-97 (2023)</p> <p>For now, sensors positions are equal across all layers.</p> PARAMETER  DESCRIPTION <code>coordinate_dim</code> <p>Dimension of coordinate space</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>num_channels</code> <p>Number of channels</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>depth</code> <p>Number of hidden layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>kernel_width</code> <p>Width of kernel network</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>kernel_depth</code> <p>Depth of kernel network</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def __init__(\nself,\ncoordinate_dim: int = 1,\nnum_channels: int = 1,\ndepth: int = 1,\nkernel_width: int = 32,\nkernel_depth: int = 3,\n):\nsuper().__init__()\nself.coordinate_dim = coordinate_dim\nself.num_channels = num_channels\nself.lifting = ContinuousConvolution(\nNeuralNetworkKernel(kernel_width, kernel_depth),\ncoordinate_dim,\nnum_channels,\n)\nself.hidden_layers = torch.nn.ModuleList(\n[\nContinuousConvolution(\nNeuralNetworkKernel(kernel_width, kernel_depth),\ncoordinate_dim,\nnum_channels,\n)\nfor _ in range(depth)\n]\n)\nself.projection = ContinuousConvolution(\nNeuralNetworkKernel(kernel_width, kernel_depth),\ncoordinate_dim,\nnum_channels,\n)\n</code></pre>"},{"location":"api/continuity/operators/neuraloperator/#continuity.operators.neuraloperator.NeuralOperator.forward","title":"<code>forward(x, u, y)</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim).</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels).</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)</p> Source code in <code>src/continuity/operators/neuraloperator.py</code> <pre><code>def forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim).\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels).\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, y_size, coordinate_dim)\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, y_size, num_channels)\n    \"\"\"\n# Unsqueeze if no batch dim\nif len(u.shape) &lt; 3:\nbatch_size = 1\nx = x.unsqueeze(0)\nu = u.unsqueeze(0)\ny = y.unsqueeze(0)\n# Patch for 1D coordinates and one channel\nif len(x.shape) &lt; 3 and self.coordinate_dim == 1:\nx = x.unsqueeze(-1)\nif len(u.shape) &lt; 3 and self.num_channels == 1:\nu = u.unsqueeze(-1)\nif len(y.shape) &lt; 3 and self.coordinate_dim == 1:\ny = y.unsqueeze(-1)\n# Get batch size etc.\nbatch_size = u.shape[0]\nnum_sensors = u.shape[1]\ny_size = y.shape[1]\n# Check shapes\nassert x.shape[0] == batch_size\nassert y.shape[0] == batch_size\nassert x.shape[1] == num_sensors\n# Lifting layer (we use x as evaluation coordinates for now)\nv = self.lifting(x, u, x)\nassert v.shape == (batch_size, num_sensors, self.num_channels)\n# Hidden layers\nfor layer in self.hidden_layers:\n# Layer operation (with residual connection)\nv = layer(x, v, x) + v\nassert v.shape == (batch_size, num_sensors, self.num_channels)\n# Activation\nv = torch.tanh(v)\n# Projection layer\nw = self.projection(x, v, y)\nassert w.shape == (batch_size, y_size, self.num_channels)\nreturn w\n</code></pre>"},{"location":"api/continuity/operators/operator/","title":"Operator","text":"<p>In Continuity, all models for operator learning are based on the <code>Operator</code> base class.</p>"},{"location":"api/continuity/operators/operator/#continuity.operators.operator.Operator","title":"<code>Operator</code>","text":"<p>             Bases: <code>Module</code></p> <p>Operator base class.</p> <p>An operator is a neural network model that maps functions by mapping an observation to the evaluations of the mapped function at given coordinates.</p> <p>This class implements default <code>compile</code> and <code>fit</code> methods.</p>"},{"location":"api/continuity/operators/operator/#continuity.operators.operator.Operator.forward","title":"<code>forward(x, u, y)</code>  <code>abstractmethod</code>","text":"<p>Forward pass through the operator.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> RETURNS DESCRIPTION <code>Tensor</code> <p>Tensor of evaluations of the mapped function of shape (batch_size, x_size, num_channels)</p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>@abstractmethod\ndef forward(self, x: Tensor, u: Tensor, y: Tensor) -&gt; Tensor:\n\"\"\"Forward pass through the operator.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)\n    Returns:\n        Tensor of evaluations of the mapped function of shape (batch_size, x_size, num_channels)\n    \"\"\"\n</code></pre>"},{"location":"api/continuity/operators/operator/#continuity.operators.operator.Operator.compile","title":"<code>compile(optimizer, loss_fn=None)</code>","text":"<p>Compile operator.</p> PARAMETER  DESCRIPTION <code>optimizer</code> <p>Torch-like optimizer.</p> <p> TYPE: <code>Optimizer</code> </p> <code>loss_fn</code> <p>Loss function taking (x, u, y, v). Defaults to MSELoss.</p> <p> TYPE: <code>Optional[Loss]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>def compile(self, optimizer: torch.optim.Optimizer, loss_fn: Optional[Loss] = None):\n\"\"\"Compile operator.\n    Args:\n        optimizer: Torch-like optimizer.\n        loss_fn: Loss function taking (x, u, y, v). Defaults to MSELoss.\n    \"\"\"\nself.optimizer = optimizer\nself.loss_fn = loss_fn or MSELoss()\n# Move to device\nself.to(device)\n# Print number of model parameters\nnum_params = sum(p.numel() for p in self.parameters())\nprint(f\"Model parameters: {num_params}\")\n</code></pre>"},{"location":"api/continuity/operators/operator/#continuity.operators.operator.Operator.fit","title":"<code>fit(dataset, epochs, writer=None)</code>","text":"<p>Fit operator to data set.</p> PARAMETER  DESCRIPTION <code>dataset</code> <p>Data set.</p> <p> TYPE: <code>DataSet</code> </p> <code>epochs</code> <p>Number of epochs.</p> <p> TYPE: <code>int</code> </p> <code>writer</code> <p>Tensorboard-like writer for loss visualization.</p> <p> TYPE: <code>Optional[SummaryWriter]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>def fit(\nself, dataset: DataSet, epochs: int, writer: Optional[SummaryWriter] = None\n):\n\"\"\"Fit operator to data set.\n    Args:\n        dataset: Data set.\n        epochs: Number of epochs.\n        writer: Tensorboard-like writer for loss visualization.\n    \"\"\"\nfor epoch in range(epochs + 1):\nmean_loss = 0\nstart = time()\nfor i in range(len(dataset)):\nx, u, y, v = dataset[i]\ndef closure(x=x, u=u, y=y, v=v):\nself.optimizer.zero_grad()\nloss = self.loss_fn(self, x, u, y, v)\nloss.backward(retain_graph=True)\nreturn loss\nself.optimizer.step(closure)\nself.optimizer.param_groups[0][\"lr\"] *= 0.999\n# Compute mean loss\nmean_loss += self.loss_fn(self, x, u, y, v).detach().item()\nend = time()\nmean_loss /= len(dataset)\nif writer is not None:\nwriter.add_scalar(\"Loss/train\", mean_loss, epoch)\niter_per_second = len(dataset) / (end - start)\nprint(\nf\"\\rEpoch {epoch}:  loss = {mean_loss:.4e}  \"\nf\"({iter_per_second:.2f} it/s)\",\nend=\"\",\n)\nprint(\"\")\n</code></pre>"},{"location":"api/continuity/operators/operator/#continuity.operators.operator.Operator.loss","title":"<code>loss(x, u, y, v)</code>","text":"<p>Evaluate loss function.</p> PARAMETER  DESCRIPTION <code>x</code> <p>Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>u</code> <p>Tensor of sensor values of shape (batch_size, num_sensors, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> <code>y</code> <p>Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)</p> <p> TYPE: <code>Tensor</code> </p> <code>v</code> <p>Tensor of labels of shape (batch_size, x_size, num_channels)</p> <p> TYPE: <code>Tensor</code> </p> Source code in <code>src/continuity/operators/operator.py</code> <pre><code>def loss(self, x: Tensor, u: Tensor, y: Tensor, v: Tensor) -&gt; Tensor:\n\"\"\"Evaluate loss function.\n    Args:\n        x: Tensor of sensor positions of shape (batch_size, num_sensors, coordinate_dim)\n        u: Tensor of sensor values of shape (batch_size, num_sensors, num_channels)\n        y: Tensor of coordinates where the mapped function is evaluated of shape (batch_size, x_size, coordinate_dim)\n        v: Tensor of labels of shape (batch_size, x_size, num_channels)\n    \"\"\"\nreturn self.loss_fn(self, x, u, y, v)\n</code></pre>"},{"location":"api/continuity/plotting/","title":"Plotting","text":"<p>Plotting utilities for Continuity.</p>"},{"location":"api/continuity/plotting/#continuity.plotting.plot_observation","title":"<code>plot_observation(observation, ax=None)</code>","text":"<p>Plots an observation.</p> <p>Currently only supports coordinate dimensions of \\(d = 1,2\\).</p> PARAMETER  DESCRIPTION <code>observation</code> <p>Observation object</p> <p> TYPE: <code>Observation</code> </p> <code>ax</code> <p>Axis object. If None, <code>plt.gca()</code> is used.</p> <p> TYPE: <code>Optional[Axis]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuity/plotting/__init__.py</code> <pre><code>def plot_observation(observation: Observation, ax: Optional[Axis] = None):\n\"\"\"Plots an observation.\n    Currently only supports coordinate dimensions of $d = 1,2$.\n    Args:\n        observation: Observation object\n        ax: Axis object. If None, `plt.gca()` is used.\n    \"\"\"\nif ax is None:\nax = plt.gca()\nx = [s.x for s in observation.sensors]\nu = [s.u for s in observation.sensors]\ndim = x[0].shape[0]\nassert dim in [1, 2], \"Only supports `d = 1,2`\"\nif dim == 1:\nax.plot(x, u, \"k.\")\nif dim == 2:\nxx, yy = [x[0] for x in x], [x[1] for x in x]\nax.scatter(xx, yy, s=20, c=u, cmap=\"jet\")\n</code></pre>"},{"location":"api/continuity/plotting/#continuity.plotting.plot_evaluation","title":"<code>plot_evaluation(operator, observation, ax=None)</code>","text":"<p>Plots the mapped function <code>operator(observation)</code> evaluated on a \\([-1, 1]^d\\) grid.</p> <p>Currently only supports coordinate dimensions of \\(d = 1,2\\).</p> PARAMETER  DESCRIPTION <code>operator</code> <p>Operator object</p> <p> TYPE: <code>Operator</code> </p> <code>observation</code> <p>Observation object</p> <p> TYPE: <code>Observation</code> </p> <code>ax</code> <p>Axis object. If None, <code>plt.gca()</code> is used.</p> <p> TYPE: <code>Optional[Axis]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/continuity/plotting/__init__.py</code> <pre><code>def plot_evaluation(\noperator: Operator, observation: Observation, ax: Optional[Axis] = None\n):\n\"\"\"Plots the mapped function `operator(observation)` evaluated on a $[-1, 1]^d$ grid.\n    Currently only supports coordinate dimensions of $d = 1,2$.\n    Args:\n        operator: Operator object\n        observation: Observation object\n        ax: Axis object. If None, `plt.gca()` is used.\n    \"\"\"\nif ax is None:\nax = plt.gca()\ndim = observation.coordinate_dim\nassert dim in [1, 2], \"Only supports `d = 1,2`\"\nif dim == 1:\nn = 200\ny = torch.linspace(-1, 1, n, device=device).unsqueeze(-1)\nx, u = observation.to_tensors()\nv = operator(x, u, y).detach()\nax.plot(y.cpu().flatten(), v.cpu().flatten(), \"k-\")\nif dim == 2:\nn = 128\nx = np.linspace(-1, 1, n)\ny = np.linspace(-1, 1, n)\nxx, yy = np.meshgrid(x, y)\nu = observation.to_tensor().unsqueeze(0).to(device)\nx = (\ntorch.tensor(\nnp.array(\n[np.array([xx[i, j], yy[i, j]]) for i in range(n) for j in range(n)]\n),\ndtype=u.dtype,\n)\n.unsqueeze(0)\n.to(device)\n)\nu = operator(u, x).detach().cpu()\nu = np.reshape(u, (n, n))\nax.contourf(xx, yy, u, cmap=\"jet\", levels=100)\nax.set_aspect(\"equal\")\n</code></pre>"},{"location":"examples/basics/","title":"Basics","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom continuity.operators import DeepONet\n</code></pre> <pre><code># Input function (some example)\nu = lambda x: torch.sin(x)\n# Target function (the derivative of u)\nv = lambda y: torch.cos(y)\n# Plot input and target function\nL = torch.pi\nx = torch.linspace(-L, L, 1000)\nplt.plot(x, u(x), \"g-\", label=\"Input $u$\")\nplt.plot(x, v(x), \"k-\", label=\"Target $v$\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>num_sensors = 32\n# Define operator\noperator = DeepONet(num_sensors)\n# Some sensor positions\nx = torch.linspace(-L, L, num_sensors)\n# Plot sensors\nplt.plot(x, u(x), \"go\", label=\"Sensors\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Some (different) evaluation positions\ny = torch.linspace(-L, L, 42)\n# Call operator\nv_pred = operator(x, u(x), y)\n# Plot the predicted function\nv_pred = v_pred.reshape(y.shape).detach()\nplt.plot(y, v(y), \"k-\", label=\"Target $v$\")\nplt.plot(y, v_pred, \"r--\", label=\"Predicted $v$ (untrained)\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code># Dataset containing single sample\ndataset = [(x, u(x), y, v(y))]\n</code></pre> <p>Using Adam we train the neural operator for 1000 epochs. </p> <pre><code>optimizer = torch.optim.Adam(operator.parameters(), lr=1e-3)\noperator.compile(optimizer)\noperator.fit(dataset, epochs=1000)\n</code></pre> <pre>\n<code>Model parameters: 7984\nEpoch 1000:  loss = 1.5345e-04  (924.06 it/s)\n</code>\n</pre> <pre><code># Use a fine equidistant grid\ny = torch.linspace(-L, L, 1000)\n# Call operator with more points\nv_pred = operator(x, u(x), y)\n# Plot output and target function\nv_pred = v_pred.reshape(y.shape).detach()\nplt.plot(y, v(y), \"k-\", label=\"Target $v$\")\nplt.plot(y, v_pred, \"r--\", label=\"Predicted $v$\")\nplt.legend()\nplt.show()\n</code></pre> <p>That's the basics!</p>"},{"location":"examples/basics/#basics","title":"Basics","text":"<p>This example demonstrates the basics of neural operators in Continuity, including how to train it on a given operator.</p>"},{"location":"examples/basics/#problem","title":"Problem","text":"<p>Let's assume we want to learn the derivative operator \\(G: u \\mapsto \\partial_x u\\). We choose (as an example) the function \\(u\\) that maps to its derivative \\(v\\) $$ u(x) = \\sin(x) \\to \\cos(y) = v(y) $$ on the interval \\([-\\pi, \\pi]\\).</p> <p>Let's start with defining and plotting input and target function!</p>"},{"location":"examples/basics/#operator","title":"Operator","text":"<p>In order to approximate the operator \\(G\\) from above with a neural network, we set up a neural operator. A neural operator takes an input function \\(u\\) evaluated at sensor positions \\(x\\) and maps it to a function \\(v\\) evaluated at (different) evaluation positions \\(y\\): $$ v(y) = \\operatorname{Operator}\\left(x, u(x)\\right)(y). $$ In this example, we choose the DeepONet architecture with 32 sensors.</p>"},{"location":"examples/basics/#plotting","title":"Plotting","text":"<p>Before we train the neural operator, let us show how to evaluate the (untrained) neural operator and plot the result.</p>"},{"location":"examples/basics/#training","title":"Training","text":"<p>Now, let's train the neural operator!</p> <p>We define a corresponding data set, i.e., a list-like of input-output samples.</p>"},{"location":"examples/basics/#evaluating","title":"Evaluating","text":"<p>The trained operator can be evaluated at arbitrary positions, so let's plot a fine resolution of the mapping along with the target function.</p>"},{"location":"examples/physicsinformed/","title":"Physics-informed","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom continuity.operators import DeepONet\nfrom continuity.data.losses import PhysicsInformedLoss\n</code></pre> <pre><code># Input functions are polynomials\ndegree = 3\ndef polynomial(coefficients):\nreturn lambda x: sum(\ncoefficients[i] * x ** i\nfor i in range(degree+1)\n)\ndef derivative(coefficients):\nreturn lambda x: sum(\ncoefficients[i+1] * (i+1) * x ** i\nfor i in range(degree)\n)\n# Generate coefficients\nnum_functions = 3\na = torch.randn(num_functions, degree+1)\n# Plot functions\nx = torch.linspace(-1, 1, 100)\nfor i in range(num_functions):\nu = polynomial(a[i])\nplt.plot(x, u(x), label=f\"Polynomial {i+1}\")\nplt.legend()\nplt.show()\n</code></pre> <pre><code>num_sensors = 32\n# Define operator\noperator = DeepONet(num_sensors)\n# Random sensor positions\nx = torch.rand(num_sensors) * 2 - 1\n</code></pre> <pre><code># Evaluation points must equal x to compute derivatives and requires grad\nx.requires_grad = True\ndataset = []\nfor i in range(num_functions):\nu = polynomial(a[i])\nsample = (x, u(x), x, None)\ndataset.append(sample)\n</code></pre> <pre><code># Derivative\ndef pde(x, u, y, v):\nu_x = torch.autograd.grad(\nu,\nx,\ngrad_outputs=torch.ones_like(u),\nretain_graph=True,\n)[0].reshape(v.shape)\nreturn ((u_x - v)**2).mean()\nloss_fn = PhysicsInformedLoss(pde)\n</code></pre> <pre><code>optimizer = torch.optim.Adam(operator.parameters(), lr=1e-3)\noperator.compile(optimizer, loss_fn)\noperator.fit(dataset, epochs=1000)\n</code></pre> <pre>\n<code>Model parameters: 7984\nEpoch 1000:  loss = 2.8581e-05  (766.69 it/s)\n</code>\n</pre> <pre><code>fig, axs = plt.subplots(1, num_functions, figsize=(12, 3))\n# Equidistant grid\ny = torch.linspace(-1, 1, 100)\n# Call operator for polynomials\nfor i in range(num_functions):\nu = polynomial(a[i])\nv_exact = derivative(a[i])\nv_pred = operator(x, u(x), y)\n# Plot functions\nv_pred = v_pred.reshape(y.shape).detach()\naxs[i].plot(y, u(y), \"g--\", label=\"Input $u$\")\naxs[i].plot(y, v_exact(y), \"k-\", label=\"Target $v$\")\naxs[i].plot(y, v_pred, \"r--\", label=\"Predicted $v$\")\nplt.legend()\nplt.show()\n</code></pre>"},{"location":"examples/physicsinformed/#physics-informed","title":"Physics-informed","text":"<p>This example demonstrates the training of a physics-informed neural operator.</p>"},{"location":"examples/physicsinformed/#problem-statement","title":"Problem Statement","text":"<p>Let's assume we want to learn the derivative operator \\(G: u \\mapsto \\partial_x u\\). We choose a set of polynomials as input functions \\(u\\).</p>"},{"location":"examples/physicsinformed/#neural-operator","title":"Neural Operator","text":"<p>In this example, we use a DeepONet architecture with 32 sensors.</p>"},{"location":"examples/physicsinformed/#data-set","title":"Data set","text":"<p>We define the data set as a list of input functions and evaluation coordinates, but without labels.</p>"},{"location":"examples/physicsinformed/#physics-informed-loss","title":"Physics-informed loss","text":"<p>As we want to learn the derivative operator \\(G\\), we define a physics-informed loss function.</p>"},{"location":"examples/physicsinformed/#training","title":"Training","text":"<p>We train the neural operator using the physics-informed loss function. </p>"},{"location":"examples/physicsinformed/#evaluating-the-trained-operator","title":"Evaluating the trained operator","text":"<p>The trained operator can be evaluated at arbitrary positions, so let's plot a fine resolution of the mapping along with the target function.</p>"},{"location":"examples/selfsupervised/","title":"Self-supervised","text":"<pre><code>import torch\nimport matplotlib.pyplot as plt\nfrom continuity.data.datasets import Sine\nfrom continuity.operators import ContinuousConvolution\nfrom continuity.operators.common import NeuralNetworkKernel\nfrom continuity.plotting import plot_evaluation, plot_observation\n</code></pre> <pre><code>size = 4\ndataset = Sine(\nnum_sensors=32,\nsize=size,\nbatch_size=4,\n)\nprint(f\"Dataset contains {len(dataset)} batches.\")\n# Plot first sample\nfirst_batch = dataset[0]\nx, u, y, v = [a[0] for a in first_batch]\nplt.plot(y, v, 'ro', label='Label')\nplt.plot(x, u, 'k.', label='Sensors')\nplt.legend()\nplt.show()\n</code></pre> <pre>\n<code>Dataset contains 32 batches.\n</code>\n</pre> <pre><code>kernel = NeuralNetworkKernel(kernel_width=128, kernel_depth=10)\nmodel = ContinuousConvolution(kernel)\n</code></pre> <pre><code>optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nmodel.compile(optimizer)\nmodel.fit(dataset, epochs=100)\n</code></pre> <pre>\n<code>Model parameters: 165505\nEpoch 100:  loss = 3.9808e-03  (232.73 it/s)\n</code>\n</pre> <pre><code>fig, axs = plt.subplots(1, 4, figsize=(16, 3))\nfor i in range(size):\nobs = dataset.get_observation(i)\nplot_evaluation(model, obs, ax=axs[i])\nplot_observation(obs, ax=axs[i])\naxs[i].set_title(f\"$k = {i}$\")\n</code></pre> <pre><code>fig, ax = plt.subplots(1, 1, figsize=(4, 3))\ni_test = (size-1) / 2\nobs = dataset.generate_observation(i_test)\nplot_evaluation(model, obs, ax=ax)\nplot_observation(obs, ax=ax)\nax.set_title(f\"$k = {i_test}$\")\n</code></pre> <pre>\n<code>Text(0.5, 1.0, '$k = 1.5$')</code>\n</pre>"},{"location":"examples/selfsupervised/#self-supervised","title":"Self-supervised","text":"<p>This example shows how to train a neural operator on sine functions in a self-supervised manner.</p>"},{"location":"examples/selfsupervised/#setup","title":"Setup","text":"<p>Import modules.</p>"},{"location":"examples/selfsupervised/#dataset","title":"Dataset","text":"<p>Create a data set of sine waves: The <code>Sine</code> dataset generates \\(N\\) sine waves $$ f(x) = \\sin(w_k x), \\quad w_k = 1 + \\frac{k}{N-1}, \\quad k = 0, \\dots, N-1. $$ As a <code>SelfSupervisedDataset</code> it exports batches of samples for self-supervised training, namely $$ \\left(\\mathbf{x}, f(\\mathbf{x}), x_j, f(x_j)\\right), \\quad \\text{for } j = 1, \\dots, M, $$ where \\(\\mathbf{x} = (x_i)_{i=1 \\dots M}\\) are the \\(M\\) equidistantly distributed sensor positions. </p>"},{"location":"examples/selfsupervised/#operator","title":"Operator","text":"<p>We use a <code>ContinuousConvolution</code> neural operator with a slightly overparameterized <code>NeuralNetworkKernel</code> as kernel function.</p>"},{"location":"examples/selfsupervised/#training","title":"Training","text":"<p>Train the neural operator.</p>"},{"location":"examples/selfsupervised/#plotting","title":"Plotting","text":"<p>Plot model predictions for training data.</p>"},{"location":"examples/selfsupervised/#generalization","title":"Generalization","text":"<p>Plot prediction on a test sample which was not part of the training set.</p>"},{"location":"getting-started/first-steps/","title":"First Steps","text":"<p>Continuity aims to implement recent advances in learning function operators, i.e., mappings of (continuous) functions. If you are not familiar with the concepts of operator learning, the page Learning Operators should introduce the key concepts.</p>"},{"location":"getting-started/first-steps/#running-the-examples","title":"Running the Examples","text":"<p>If you are familiar with the idea of operator learning, you can start by browsing our examples illustrating Continuity's capabilities, either:</p> <ul> <li>In the examples under Learning Operators</li> <li>Locally, by starting a jupyter server at the root of the project.</li> </ul>"},{"location":"getting-started/installation/","title":"Installing Continuity","text":"<p>To install the latest development version use: <pre><code>git clone https://github.com/aai-institute/Continuity.git\ncd Continuity\npip install -e .\n</code></pre></p>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>Continuity requires Python&gt;=3.9 and is built on top of PyTorch.</p>"},{"location":"operators/","title":"Introduction","text":""},{"location":"operators/#operators","title":"Operators","text":"<p>In mathematics, operators are function mappings \u2013 they map functions to functions.</p> <p>Let \\(u: \\mathbb{R}^d \\to \\mathbb{R}^c\\) be a function that maps a \\(d\\)-dimensional input to \\(c\\) channels. Then, an operator $$ G: u \\to v $$ maps \\(u\\) to a function \\(v: \\mathbb{R}^{d'} \\to \\mathbb{R}^{c'}\\).</p> <p>Example</p> <p>The operator \\(G: u \\to \\partial_x u\\) maps functions \\(u\\) to their partial derivative \\(\\partial_x u\\).</p>"},{"location":"operators/#learning-operators","title":"Learning Operators","text":"<p>Learning operators is the task of learning the mapping \\(G\\) from data. In the context of neural networks, we want to learn a neural network \\(G_\\theta\\) with parameters \\(\\theta\\) that, given a set of input-output pairs \\((u_k, v_k)\\), maps \\(u_k\\) to \\(v_k\\).</p> <p>As neural networks take vectors as input, we need to vectorize the input function \\(u\\) somehow. There are two possibilities:</p> <ol> <li>We represent the function \\(u\\) within a finite-dimensional function space   (e.g. the space of polynomials) and map the coefficients, or</li> <li>We map evaluations of the function at a finite set of evaluation points.</li> </ol> <p>In Continuity, we use the second, more geneal approach of mapping function evaluations, and use this also for the representation of the output function \\(v\\).</p> <p>In the input domain, we evaluate the function \\(u\\) at a set of points \\(x_i\\) and collect a set of sensors \\((x_i, u(x_i))\\) in an observation $$ \\mathcal{O} = \\{ (x_i, u(x_i)) \\mid i = 1, \\dots N \\}. $$</p> <p>The mapped function can then be evaluated at query points \\(\\mathbf{y}\\) to obtain the output $$ v(\\mathbf{y}) = G(u)(\\mathbf{y}) \\approx G_\\theta(\\mathbf{x}, \\mathbf{u}; \\mathbf{y}) = \\mathbf{v} $$ where \\(\\mathbf{x} = (x_i)_i\\) and \\(\\mathbf{y} = (y_j)_j\\) are the evaluation points of the input and output domain, respectively, and \\(\\mathbf{u} = (u_i)_i\\) is the vector of function evaluations at \\(\\mathbf{x}\\). The output \\(\\mathbf{v} = (v_j)_j\\) is the vector of function evaluations at \\(\\mathbf{y}\\).</p> <p>In Python, this call can be written like <pre><code>v = operator(x, u, y)\n</code></pre></p>"},{"location":"operators/#applications-to-pdes","title":"Applications to PDEs","text":"<p>Operators are ubiquitous in mathematics and physics. They are used to describe the dynamics of physical systems, such as the Navier-Stokes equations in fluid dynamics. As solutions of PDEs are functions, it is natural to use the concept of neural operators to learn solution operators of PDEs. One possibility to do this is using an inductive bias, or physics-informed training. See our examples in Learning Operators for more details.</p>"}]}